import numpy as np

import torch
import transformers
import scipy
from transformer_utils.logit_lens import make_lens_hooks
from transformer_utils.logit_lens.layer_names import make_layer_names

def postprocess_logits(layer_logits):
    layer_preds = layer_logits.argmax(axis=-1)

    layer_probs = scipy.special.softmax(layer_logits, axis=-1)

    return layer_preds, layer_probs

def collect_logits(model, input_ids, layer_names, decoder_layer_names):
    model._last_resid = None

    with torch.no_grad():
        out = model(input_ids)
    del out
    model._last_resid = None

    layer_logits = np.concatenate(
        [model._layer_logits[name] for name in layer_names],
        axis=0,
    )

    return layer_logits, layer_names


model_large = transformers.AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M')
model_small = transformers.AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-2Layers-33M')
tokenizer = transformers.AutoTokenizer.from_pretrained('roneneldan/TinyStories-33M')


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_large.to(device)
model_small.to(device)
test_text_from_gpt4="""Once upon a time, in a small, old house, lived a little boy named Tom and his cat, Whiskers. The house was very old, so old that everyone in the town called it the "ancient house." Tom loved his ancient house very much."""


def text_to_input_ids(text):
    toks = tokenizer.encode(text)
    return torch.as_tensor(toks).view(1, -1).cuda()

input_ids = text_to_input_ids(test_text_from_gpt4)
input_ids = input_ids.to(device)




block_step=1
include_input=True
force_include_output=True
decoder_layer_names = ['final_layernorm', 'lm_head']
verbose = True
include_subblocks = True

def get_logits(model, input_ids):

    layer_names = make_layer_names(
        model,
        block_step=block_step,
        include_input=include_input,
        force_include_output=force_include_output,
        include_subblocks=include_subblocks,
        decoder_layer_names=decoder_layer_names
    )
    start_ix=0
    end_ix=input_ids.shape[1] - 1 #,#len(input_ids) - 1

    make_lens_hooks(model, start_ix=start_ix, end_ix=end_ix, layer_names=layer_names,
                    decoder_layer_names=decoder_layer_names,
                        verbose=verbose)
                        
    layer_logits, layer_names = collect_logits(
        model, input_ids, layer_names=layer_names, decoder_layer_names=decoder_layer_names,
    )

    layer_preds, layer_probs = postprocess_logits(layer_logits)
    return layer_logits, layer_names, layer_preds, layer_probs

#layer_logits_small, layer_names_small, layer_preds_small, layer_probs_small = get_logits(model_small)
#layer_logits_large, layer_names_large, layer_preds_large, layer_probs_large = get_logits(model_large)
"""
print("layer_logits_small")
print(layer_logits_small)
print("layer_names_small")
print(layer_names_small)
print("layer_preds_small")
print(layer_preds_small)
print("layer_probs_small")
print(layer_probs_small)


print("layer_logits_large")
print(layer_logits_large)
print("layer_names_large")
print(layer_names_large)
print("layer_preds_large")
print(layer_preds_large)
print("layer_probs_large")
print(layer_probs_large)
"""
"""
def print_last_layer_info(layer_logits, layer_names, tokenizer, input_ids):
    last_layer_logits = torch.tensor(layer_logits[-1])  # Convert to tensor if it's a numpy array
    top_preds = torch.argmax(last_layer_logits, dim=-1)  # Find the top predictions

    for i, logits in enumerate(last_layer_logits.squeeze()):
        token = tokenizer.decode(input_ids[0, i])  # Decode the current token
        next_token = tokenizer.decode(top_preds[i].item())  # Decode the predicted next token

        print(f"Token: '{token}' - Predicted next token: '{next_token}'")

# Usage for each model
print("Small Model Predictions:")
print_last_layer_info(layer_logits_small, layer_names_small, tokenizer, input_ids)

print("\nLarge Model Predictions:")
print_last_layer_info(layer_logits_large, layer_names_large, tokenizer, input_ids)
def compare_and_print_predictions(layer_logits_small, layer_logits_large, input_ids, tokenizer):
    last_layer_logits_small = torch.tensor(layer_logits_small[-1])
    last_layer_logits_large = torch.tensor(layer_logits_large[-1])

    top_preds_small = torch.argmax(last_layer_logits_small, dim=-1)
    top_preds_large = torch.argmax(last_layer_logits_large, dim=-1)

    for i in range(input_ids.shape[1] - 1):  # Exclude the last token as it has no next token
        current_token = tokenizer.decode(input_ids[0, i])
        actual_next_token = tokenizer.decode(input_ids[0, i+1])
        predicted_next_token_small = tokenizer.decode(top_preds_small[i].item())
        predicted_next_token_large = tokenizer.decode(top_preds_large[i].item())

        if predicted_next_token_large == actual_next_token and predicted_next_token_small != actual_next_token:
            print(f"Token: '{current_token}' - Small Model Predicted: '{predicted_next_token_small}', Large Model Correctly Predicted: '{predicted_next_token_large}'")

# Usage
compare_and_print_predictions(layer_logits_small, layer_logits_large, input_ids, tokenizer)

"""
"""
def compare_and_print_predictions(layer_logits_small, layer_logits_large, input_ids, tokenizer):
    last_layer_logits_small = torch.tensor(layer_logits_small[-1])
    last_layer_logits_large = torch.tensor(layer_logits_large[-1])

    top_preds_small = torch.argmax(last_layer_logits_small, dim=-1)
    top_preds_large = torch.argmax(last_layer_logits_large, dim=-1)

    # Ensure we do not go out of bounds
    num_tokens = min(input_ids.shape[1] - 1, top_preds_small.shape[0], top_preds_large.shape[0])

    for i in range(num_tokens):  # Iterate only over the valid range
        current_token = tokenizer.decode(input_ids[0, i])
        actual_next_token = tokenizer.decode(input_ids[0, i+1])
        predicted_next_token_small = tokenizer.decode(top_preds_small[i].item())
        predicted_next_token_large = tokenizer.decode(top_preds_large[i].item())

        if predicted_next_token_large == actual_next_token and predicted_next_token_small != actual_next_token:
            print(f"Token: '{current_token}' - Small Model Predicted: '{predicted_next_token_small}', Large Model Correctly Predicted: '{predicted_next_token_large}'")

# Usage
compare_and_print_predictions(layer_logits_small, layer_logits_large, input_ids, tokenizer)
"""
"""
import torch
import torch.nn.functional as F

def compare_and_print_predictions_with_probs(layer_logits_small, layer_logits_large, input_ids, tokenizer):
    last_layer_logits_small = torch.tensor(layer_logits_small[-1])
    last_layer_logits_large = torch.tensor(layer_logits_large[-1])

    top_preds_small = torch.argmax(last_layer_logits_small, dim=-1)
    top_preds_large = torch.argmax(last_layer_logits_large, dim=-1)

    # Convert logits to probabilities using softmax
    probs_small = F.softmax(last_layer_logits_small, dim=-1)
    probs_large = F.softmax(last_layer_logits_large, dim=-1)

    num_tokens = min(input_ids.shape[1] - 1, top_preds_small.shape[0], top_preds_large.shape[0])

    for i in range(num_tokens):  # Iterate only over the valid range
        current_token = tokenizer.decode(input_ids[0, i])
        actual_next_token = tokenizer.decode(input_ids[0, i+1])
        predicted_next_token_small = tokenizer.decode(top_preds_small[i].item())
        predicted_next_token_large = tokenizer.decode(top_preds_large[i].item())

        if predicted_next_token_large == actual_next_token and predicted_next_token_small != actual_next_token:
            prob_small = probs_small[i, top_preds_small[i]].item()
            prob_large = probs_large[i, top_preds_large[i]].item()

            print(f"Token: '{current_token}' - Small Model Predicted: '{predicted_next_token_small}' (Prob: {prob_small:.4f}), Large Model Correctly Predicted: '{predicted_next_token_large}' (Prob: {prob_large:.4f})")

# Usage
compare_and_print_predictions_with_probs(layer_logits_small, layer_logits_large, input_ids, tokenizer)
"""
import torch
2import torch.nn.functional as F

def compare_and_print_predictions_with_context_and_probs(layer_logits_small, layer_logits_large, input_ids, tokenizer):
    last_layer_logits_small = torch.tensor(layer_logits_small[-1])
    last_layer_logits_large = torch.tensor(layer_logits_large[-1])

    top_preds_small = torch.argmax(last_layer_logits_small, dim=-1)
    top_preds_large = torch.argmax(last_layer_logits_large, dim=-1)

    # Convert logits to probabilities using softmax
    probs_small = F.softmax(last_layer_logits_small, dim=-1)
    probs_large = F.softmax(last_layer_logits_large, dim=-1)

    num_tokens = min(input_ids.shape[1] - 1, top_preds_small.shape[0], top_preds_large.shape[0])
    accumulated_string = ""
    #print("num_tokens")
    #print(num_tokens)
    for i in range(num_tokens):  # Iterate only over the valid range
        current_token = tokenizer.decode(input_ids[0, i])
        accumulated_string += current_token + " "
        actual_next_token = tokenizer.decode(input_ids[0, i+1])
        predicted_next_token_small = tokenizer.decode(top_preds_small[i].item())
        predicted_next_token_large = tokenizer.decode(top_preds_large[i].item())

        if predicted_next_token_large == actual_next_token and predicted_next_token_small != actual_next_token:
            prob_small = probs_small[i, top_preds_small[i]].item()
            prob_large = probs_large[i, top_preds_large[i]].item()
            prob_large_for_small_pred = probs_large[i, top_preds_small[i]].item()

            if ( (prob_large - prob_small) > .3):
                extended_context_end = min(i + 11, input_ids.shape[1])  # Ensure not to exceed total tokens
                extended_context = tokenizer.decode(input_ids[0, :extended_context_end])
                
                print(f"Context: {accumulated_string.strip()}")
                print(f"Token: '{current_token}' - Small Model Predicted: '{predicted_next_token_small}' (Prob: {prob_small:.4f}), Large Model Correctly Predicted: '{predicted_next_token_large}' (Prob: {prob_large:.4f})")
                print(f"Probability assigned by Large Model to Small Model's prediction: {prob_large_for_small_pred:.4f}")
                print()

# Usage

examples = ["Alice was so tired when she got back home so she went straight to bed. ",
"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked",
"Alice and Jack walked up the street and met a girl in a red dress. The girl said to them, \"Hi, I'm Jane. What are your names?\" Alice said, \"I'm Alice and this is Jack.\"",
"Jack and Lily saw a rainbow after a rainy day. They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has red, orange, yellow, green, blue, and purple!\"",
"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was small and thin.",
"Jack told Mary, \"If you give me your banana, I'll give you my apple\". Mary gave Jack her Banana so he could give her the apple.",
"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to his grandmother's house,"]

def print_better_predictions(examples):
    for example in examples:
        print("")
        print("")
        print("new example")
        print("")
        input_ids = text_to_input_ids(example)
        input_ids = input_ids.to(device)

        start_ix=0
        end_ix=input_ids.shape[1] - 1#,#len(input_ids) - 1

        layer_logits_small, layer_names_small, layer_preds_small, layer_probs_small = get_logits(model_small, input_ids)
        layer_logits_large, layer_names_large, layer_preds_large, layer_probs_large = get_logits(model_large, input_ids)
        compare_and_print_predictions_with_context_and_probs(layer_logits_small, layer_logits_large, input_ids, tokenizer)

if __name__ == "__main__":
    print_better_predictions(examples)
